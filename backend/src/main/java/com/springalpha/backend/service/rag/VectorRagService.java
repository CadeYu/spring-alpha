package com.springalpha.backend.service.rag;

import lombok.extern.slf4j.Slf4j;
import org.springframework.ai.document.Document;
import org.springframework.ai.vectorstore.SearchRequest;
import org.springframework.ai.vectorstore.VectorStore;
import org.springframework.stereotype.Service;

import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * Vector RAG Service - å‘é‡å¢å¼ºæ£€ç´¢æœåŠ¡
 * <p>
 * è¿™æ˜¯ RAG (Retrieval-Augmented Generation) çš„æ ¸å¿ƒå¼•æ“ã€‚
 * å®ƒè´Ÿè´£è¿æ¥æ­¤æ—¶æ­¤åˆ»çš„ "User Query" å’Œæµ©ç€šçš„ "SEC 10-K Document"ã€‚
 * <p>
 * **æ ¸å¿ƒæµç¨‹**:
 * 1. **Ingestion (å…¥åº“)**: æŠŠ 10MB çš„æ–‡æœ¬åˆ‡æˆå°å— (Chunking)ï¼Œç®—æˆå‘é‡ï¼Œå­˜å…¥ PGVectorã€‚
 * 2. **Retrieval (æ£€ç´¢)**: å½“ç”¨æˆ·é—® "ä¸ºä»€ä¹ˆäºæŸ" æ—¶ï¼ŒæŠŠè¿™ä¸ªé—®é¢˜ä¹Ÿå˜æˆå‘é‡ï¼Œå»æ•°æ®åº“é‡Œæ‰¾æœ€ç›¸ä¼¼çš„ 5 ä¸ªç‰‡æ®µã€‚
 */
@Slf4j
@Service
public class VectorRagService {

    // åªå–æœ€ç›¸å…³çš„ 5 ä¸ªç‰‡æ®µï¼Œå¤§çº¦ 2000-3000 tokensï¼Œåˆšå¥½å¡«æ»¡ LLM çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œåˆä¸ä¼šå¤ªè´µ
    private static final int TOP_K = 5;

    // ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0 - 1.0)ã€‚
    // é™ä½åˆ° 0.4 æ˜¯ä¸ºäº†æé«˜"å¬å›ç‡" (Recall)ï¼Œå³ä½¿æªè¾ä¸å®Œå…¨ä¸€æ ·ä¹Ÿèƒ½æœåˆ°ã€‚
    // æ¯”å¦‚ï¼šQuery="risk" èƒ½æœåˆ° Content="uncertainty"
    private static final double SIMILARITY_THRESHOLD = 0.4;

    // æ¯ä¸ªåˆ‡ç‰‡çš„å¤§å° (å­—ç¬¦æ•°)ã€‚3000 chars â‰ˆ 700 tokensã€‚
    private static final int CHUNK_SIZE = 3000;

    // åˆ‡ç‰‡é‡å éƒ¨åˆ†ï¼Œé˜²æ­¢ä¸€å¥è¯è¢«åˆ‡æ–­å¯¼è‡´è¯­ä¹‰ä¸¢å¤±ã€‚
    private static final int CHUNK_OVERLAP = 300;

    // é™åˆ¶æœ€å¤§åˆ‡ç‰‡æ•°ï¼Œé˜²æ­¢ä¸€ç¯‡è¶…é•¿æ–‡æ¡£æŠŠæ•°æ®åº“æ’‘çˆ†ï¼Œæˆ–è€… Embedding API è¶…æ—¶ã€‚
    private static final int MAX_CHUNKS = 30;

    private final VectorStore vectorStore;

    public VectorRagService(VectorStore vectorStore) {
        this.vectorStore = vectorStore;
        log.info("âœ… VectorRagService initialized with PGVector store");
    }

    /**
     * å­˜å‚¨è´¢æŠ¥æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
     * <p>
     * è¿™æ˜¯ä¸€ä¸ª **è€—æ—¶æ“ä½œ** (Embedding API è°ƒç”¨ + DB å†™å…¥)ï¼Œé€šå¸¸å¼‚æ­¥æ‰§è¡Œã€‚
     * 
     * @param ticker  è‚¡ç¥¨ä»£ç 
     * @param content æ¸…æ´—åçš„ SEC 10-K æ–‡æœ¬ (Markdown æ ¼å¼)
     */
    public void storeDocument(String ticker, String content) {
        log.info("ğŸ“¥ Storing document for ticker: {} ({} chars)", ticker, content.length());

        // 1. åˆ‡ç‰‡ (Chunking): æŠŠå¤§è±¡æ”¾è¿›å†°ç®±çš„ç¬¬ä¸€æ­¥ï¼ŒæŠŠæ–‡æœ¬åˆ‡ç¢
        List<String> chunks = splitIntoChunks(content);
        log.info("âœ‚ï¸ Split into {} chunks (CHUNK_SIZE={}, MAX_CHUNKS={})", chunks.size(), CHUNK_SIZE, MAX_CHUNKS);

        // 2. åŒ…è£… (Wrapping): æŠŠæ–‡æœ¬å—åŒ…è£…æˆ Document å¯¹è±¡ï¼Œå¹¶æ‰“ä¸Š Metadata æ ‡ç­¾(ticker)
        // è¿™æ ·æ£€ç´¢æ—¶å¯ä»¥é€šè¿‡ metadata filter åªæœç‰¹å®šå…¬å¸çš„æ–‡æ¡£
        List<Document> documents = chunks.stream()
                .map(chunk -> new Document(chunk, Map.of(
                        "ticker", ticker,
                        "source", "sec-10k")))
                .collect(Collectors.toList());

        log.info("ğŸ”¢ Calling vectorStore.add() with {} documents. This will generate embeddings (may take a while)...",
                documents.size());

        // 3. å‘é‡åŒ–ä¸å­˜å‚¨ (Embedding & Upsert):
        // è¿™é‡Œä¼šè°ƒç”¨ EmbeddingModel (Gemini/OpenAI) æŠŠæ–‡æœ¬å˜æˆå‘é‡
        // ç„¶åå­˜å…¥ Postgres çš„ vector å­—æ®µ
        long startTime = System.currentTimeMillis();
        vectorStore.add(documents);

        long elapsed = System.currentTimeMillis() - startTime;
        log.info("âœ… Stored {} document chunks for {} in {} ms", documents.size(), ticker, elapsed);
    }

    /**
     * è¯­ä¹‰æ£€ç´¢ (Semantic Search)
     * <p>
     * è¿™æ˜¯ RAG çš„ "Retrieve" æ­¥éª¤ã€‚
     * å®ƒä¸ä»…ä»…æ˜¯å…³é”®è¯åŒ¹é… (Keyword Match)ï¼Œè€Œæ˜¯ç†è§£è¯­ä¹‰ã€‚
     * 
     * @param ticker è‚¡ç¥¨ä»£ç  (ä½œä¸º Filter)
     * @param query  ç”¨æˆ·çš„é—®é¢˜ (e.g., "What are the revenue drivers?")
     * @return æ‹¼æ¥å¥½çš„ç›¸å…³æ–‡æœ¬ç‰‡æ®µï¼Œç”¨ä½œ Prompt çš„ context
     */
    public String retrieveRelevantContext(String ticker, String query) {
        log.info("ğŸ” Searching for: '{}' in ticker: {}", query, ticker);

        SearchRequest searchRequest = SearchRequest.builder()
                .query(query)
                .topK(TOP_K) // åªæˆ‘ä»¬è¦æœ€ç›¸å…³çš„ K ä¸ª
                .similarityThreshold(SIMILARITY_THRESHOLD) // è¿‡æ»¤æ‰ä¸ç›¸å…³çš„å™ªéŸ³
                .filterExpression("ticker == '" + ticker + "'") // å…³é”®ï¼šåªåœ¨å½“å‰è‚¡ç¥¨çš„æ–‡æ¡£é‡Œæœï¼
                .build();

        List<Document> results = vectorStore.similaritySearch(searchRequest);

        if (results.isEmpty()) {
            log.warn("No relevant documents found for query: {}", query);
            return "";
        }

        log.info("ğŸ“„ Found {} relevant chunks", results.size());

        // å°†æœåˆ°çš„ç‰‡æ®µç”¨åˆ†éš”ç¬¦æ‹¼èµ·æ¥ï¼Œå–‚ç»™ LLM
        return results.stream()
                .map(Document::getFormattedContent)
                .collect(Collectors.joining("\n\n---\n\n"));
    }

    /**
     * æ£€æŸ¥æ˜¯å¦å·²å­˜å‚¨è¯¥ ticker çš„æ–‡æ¡£ (å¹‚ç­‰æ€§æ£€æŸ¥)
     * é˜²æ­¢é‡å¤å¤„ç†åŒä¸€ä¸ªæ–‡ä»¶
     */
    public boolean hasDocuments(String ticker) {
        // éšä¾¿æœä¸€ä¸ªè¯ï¼Œçœ‹çœ‹æœ‰æ²¡æœ‰ç»“æœï¼Œæœ‰å°±æ˜¯å­˜è¿‡äº†
        SearchRequest searchRequest = SearchRequest.builder()
                .query("financial report")
                .topK(1)
                .filterExpression("ticker == '" + ticker + "'")
                .build();

        return !vectorStore.similaritySearch(searchRequest).isEmpty();
    }

    /**
     * é€’å½’åˆ‡ç‰‡ç®—æ³• (Simple Recursive Splitter)
     * <p>
     * å°½é‡åœ¨å¥å­ç»“æŸç¬¦ (. ) å¤„åˆ‡åˆ†ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´ã€‚
     */
    private List<String> splitIntoChunks(String text) {
        List<String> chunks = new java.util.ArrayList<>();

        if (text == null || text.isEmpty()) {
            return chunks;
        }

        int start = 0;
        while (start < text.length() && chunks.size() < MAX_CHUNKS) {
            int end = Math.min(start + CHUNK_SIZE, text.length());

            // ä¼˜åŒ–ï¼šå°è¯•åœ¨å¥å­å¥å·å¤„æ–­å¥ï¼Œè€Œä¸æ˜¯ç”Ÿç¡¬åœ°åˆ‡æ–­
            if (end < text.length()) {
                int lastPeriod = text.lastIndexOf(". ", end);
                if (lastPeriod > start + CHUNK_SIZE / 2) {
                    end = lastPeriod + 1;
                }
            }

            String chunk = text.substring(start, end).trim();
            if (!chunk.isEmpty()) {
                chunks.add(chunk);
            }

            // å…³é”®ï¼šç¡®ä¿ start æ°¸è¿œå‘å‰ç§»åŠ¨ï¼Œé˜²æ­¢æ­»å¾ªç¯
            int nextStart = end - CHUNK_OVERLAP; // åˆ¶é€ é‡å ï¼Œä¿è¯ä¸Šä¸‹æ–‡è¿è´¯
            if (nextStart <= start) {
                nextStart = start + 1; // å¼ºåˆ¶å‰è¿›
            }
            start = nextStart;
        }

        if (chunks.size() >= MAX_CHUNKS) {
            log.warn("âš ï¸ Document truncated to {} chunks to prevent OOM", MAX_CHUNKS);
        }

        return chunks;
    }
}
